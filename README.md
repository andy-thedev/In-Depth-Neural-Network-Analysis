# In-Depth-Neural-Network-Analysis

A repository containing codes from scratch, with the purpose of understanding how neural networks work, beginning with decisive fundamentals such as: 
formulas, structures, and mechanisms that ultimately build a neural network.

Here are the topics analyzed for each file listed in this repository:

Understanding_NN_1.py

  Analyzes a singular connection from scratch, with simple linear calculations of weights and biases to determine an output.

  Disregards OOP and sophistication, as following files will build upon each recreated technique/knowledge

Understanding_NN_2.py

  Analyzes a single layer from scratch, with the same linear calculations of weights and biases per neuron, as observed in the previous file.

  Disregards OOP and sophistication, as following files will build upon each recreated technique/knowledge

Understanding_NN_3.py

  Cleaning up Understanding_NN_2.py using numpy, as well as loops to enforce diverse methods.

  Disregards OOP and sophistication, as following files will build upon each recreated technique/knowledge

Understanding_NN_4.py

  Utilizing, and understanding the use of batches of inputs for neural networks. Calculations and structure is congruent to previous recreations.

  Disregards OOP and sophistication, as following files will build upon each recreated technique/knowledge

Understanding_NN_5.py

  Utilizing Object Oriented Programming to code an arbitrary neural network (forward-pass), with previously analyzed formulas and techniques, such as batch inputs.

Understanding_NN_6.py

  Analyzes activation functions, such as Step, Sigmoid, and Rectified Linear Unit (ReLU) functions, and combines it with the OOP design from the previous file.
  
  ReLU is specifically implemented (OOP), as it is very popular for hidden layers in neural networks.

